#!/usr/bin/env python

'''
zaobao.com.sg - 仅获取中国和国际板块
'''

from calibre.web.feeds.news import BasicNewsRecipe, classes
from urllib.parse import urljoin


class ZAOBAO(BasicNewsRecipe):
    title = '联合早报 - 中国与国际'
    __author__ = 'unkn0wn'
    description = '联合早报 - 仅获取中国和国际新闻，包括即时和新闻板块'
    no_stylesheets = True
    language = 'zh'
    encoding = 'utf-8'
    masthead_url = (
        'https://upload.wikimedia.org/wikipedia/en/2/29/Lianhe_Zaobao_new_logo.svg'
    )
    remove_javascript = True
    ignore_duplicate_articles = {'url'}
    remove_attributes = ['width', 'height', 'style', 'data-*']
    resolve_internal_links = True
    remove_empty_feeds = True

    extra_css = '''
        .calibre-nuked-tag-figcaption {font-size:small; text-align:center; }
        img {display:block; margin:0 auto; max-width: 100%;}
        .article-body { line-height: 1.6; }
    '''

    def get_cover_url(self):
        try:
            soup = self.index_to_soup(
                'https://frontpages.freedomforum.org/newspapers/sing_lz-Lianhe_Zaobao'
            )
            img_tag = soup.find(
                'img',
                attrs={
                    'alt': 'Front Page Image',
                    'src': lambda x: x and x.endswith('front-page-large.jpg'),
                },
            )
            if img_tag:
                return img_tag['src'].replace('-large', '-medium')
        except Exception:
            pass
        # 备用封面
        return 'https://static.zaobao.com.sg/s3fs-public/2023/08/zaobao-logo.jpg'

    # 修复：使用正确的选择器来保留文章内容
    keep_only_tags = [
        dict(name='div', attrs={'class': ['story__content', 'article-body', 'article-content']}),
        dict(name='article'),
    ]

    remove_tags = [
        classes('bff-google-ad'),
        classes('bff-recommend-article'),
        dict(name=['aside', 'nav', 'footer', 'script', 'style', 'div'], 
             attrs={'class': ['advertisement', 'ad-container', 'google-ads', 'related-articles', 'share-tools']}),
    ]

    # 添加更多配置以提高抓取成功率
    publication_type = 'newspaper'
    oldest_article = 7
    max_articles_per_feed = 10  # 每个板块最多20篇文章
    auto_cleanup = True

    def parse_index(self):
        index = 'https://www.zaobao.com.sg'
        
        # 只获取你指定的4个板块
        sections = [
            ('中国即时', f'{index}/realtime/china'),
            ('国际即时', f'{index}/realtime/world'),
            ('中国新闻', f'{index}/news/china'),
            ('国际新闻', f'{index}/news/world'),
        ]

        feeds = []

        for section_name, section_url in sections:
            self.log(f"正在获取: {section_name}")
            
            try:
                soup = self.index_to_soup(section_url)
            except Exception as e:
                self.log(f"无法获取板块 {section_name}，跳过: {e}")
                continue

            articles = []
            url_set = set()  # 防止重复文章

            # 查找文章链接 - 使用更灵活的选择器
            for a in soup.find_all(
                'a',
                href=lambda x: x and (
                    x.startswith('/realtime/china/') or 
                    x.startswith('/realtime/world/') or 
                    x.startswith('/news/china/') or 
                    x.startswith('/news/world/')
                )
            ):
                # 跳过包含图片的链接（通常是推荐或广告）
                if a.find('img') or 'video' in a.get('href', '') or 'gallery' in a.get('href', ''):
                    continue
                
                title_elem = a.find(['h1', 'h2', 'h3', 'span', 'div'])
                title = self.tag_to_string(title_elem) if title_elem else self.tag_to_string(a)
                title = title.strip()
                
                if not title:
                    continue
                
                # 使用 urljoin 处理相对链接
                url = urljoin(index, a['href'])
                
                # 避免重复
                if url in url_set:
                    continue
                url_set.add(url)
                
                self.log(f'\t{title} -> {url}')
                articles.append({'title': title, 'url': url})
            
            # 限制每板块文章数量
            if articles:
                feeds.append((section_name, articles[:self.max_articles_per_feed]))
        
        return feeds



